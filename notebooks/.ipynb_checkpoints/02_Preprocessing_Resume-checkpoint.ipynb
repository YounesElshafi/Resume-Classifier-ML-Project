{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fafae09-5d75-4e0f-9da3-e9c44a92708a",
   "metadata": {},
   "source": [
    "# ðŸ§¹ Data Preprocessing & Feature Extraction for Resume Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa75353-ddb8-4716-84f9-9b1d3c0c68c0",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2f006f30-cb5f-4f7b-bce7-e03c7d352093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import joblib\n",
    "import string\n",
    "import spacy\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f048d584-b266-4bda-aff7-07f365b55307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\usife\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3444a-12ae-4a52-9ab3-032b6ab2d29a",
   "metadata": {},
   "source": [
    "## 2. Load The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "be76474e-4473-435c-8807-8462c47ed899",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/resume-dataset/UpdatedResumeDataSet.csv'  \n",
    "DataSet = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "016069e2-1622-47a7-a352-04188486ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates if not done already\n",
    "DataSet.drop_duplicates(subset=['Resume'], inplace=True)\n",
    "DataSet.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18fd5a5-be5c-4e86-a1b6-e5e21d65cac4",
   "metadata": {},
   "source": [
    "## 3. Create a Copy for Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ee72437a-c641-4ecc-98f9-dd8b7b9ae10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_DataSet = DataSet.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0db55-f617-4fd2-b8e8-0af0e4e8cc12",
   "metadata": {},
   "source": [
    "## 4. Define Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e339e14e-2e79-4b32-bcb4-48c7472032de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "00233f2a-50ee-4062-bb33-820f50027e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common non-informative or noisy words to remove\n",
    "common_garbage = [\n",
    "    'skills', 'skill', 'skill details', 'education', 'education details',\n",
    "    'company', 'company details', 'description', 'details', 'project',\n",
    "    'months', 'exprience', 'year', 'january', 'client',\n",
    "    'pvt', 'ltd', 'responsibilities', 'technical', 'environment',\n",
    "    'work', 'working', 'like', 'using', 'various',\n",
    "    'maharashtra', 'pune', 'india', 'monthscompany',\n",
    "    'university', 'college', 'experience', 'team', 'role', 'work',\n",
    "    'maintain', 'support', 'handle', 'activity', 'window', 'time',\n",
    "    'good', 'new', 'etc', 'requirement', 'provide',\n",
    "    'detail', 'month', 'base', 'high', 'responsibility', 'issue',\n",
    "    'plan', 'user', 'office', 'include', 'relate', 'level', 'job',\n",
    "    'information', '2016', 'ssc', 'mumbai',\n",
    "    'school', 'bachelor', '2017', 'different', 'involve', 'document',\n",
    "    'complete', 'quality', 'ensure', 'diploma', 'institute', '2015', 'control',\n",
    "    'datum', '2014', '2012', 'hsc'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "67a4f21a-53ac-48f5-a2c2-4ab959115a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for standardizing skill names\n",
    "standardize_skills = {\n",
    "    'scikit learn': 'scikit-learn',\n",
    "    'matplot lib': 'matplotlib',\n",
    "    'sqlserver': 'sql-server',\n",
    "    'doc vec': 'doc2vec',\n",
    "    'word vec': 'word2vec',\n",
    "    'vader': 'VADER',\n",
    "    'text blob': 'TextBlob',\n",
    "    'nodejs': 'NodeJS'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "284214b9-0182-480e-bd10-cf2ff45913a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_resume(text):\n",
    "    \"\"\"\n",
    "    Cleans and standardizes resume text for NLP processing.\n",
    "    Steps include lowercasing, removing noise, standardizing terms,\n",
    "    and lemmatization with filtering.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # clening use Remove URLs, emails, mentions\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "    text = re.sub(r'\\b(?:' + '|'.join(map(re.escape, common_garbage)) + r')\\b', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'@[\\w_]+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'\\r|\\n|\\r\\n', ' ', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    text = re.sub(r'(js){2,}', 'js', text)  \n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)  \n",
    "\n",
    "\n",
    "    # Standardize skill variants \n",
    "    for k, v in standardize_skills.items():\n",
    "        text = text.replace(k, v)\n",
    "\n",
    "    # Lemmatize and remove stopwords, punctuation, and short tokens\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop and not token.is_punct and len(token) > 2\n",
    "    ]\n",
    "\n",
    "    # Remove duplicates while preserving token order\n",
    "    seen = set()\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in seen:\n",
    "            seen.add(token)\n",
    "            cleaned_tokens.append(token)\n",
    "\n",
    "    return ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa528759-7c3f-4b98-8d01-838186b870c7",
   "metadata": {},
   "source": [
    "## 5. Apply Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f8472566-fd8a-4aa7-bfed-b6b827f79939",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_DataSet['Resume'] = cleaned_DataSet['Resume'].apply(clean_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "88469f4c-3ec8-4c4a-84a7-afe4611cafca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Sample Resume Text After cleaning:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'programming language python pandas numpy scipy scikitlearn matplotlib sql java javascriptjquery machine learn regression svm naa baye knn random forest decision tree boost technique cluster analysis word embed sentiment natural process dimensionality reduction topic model lda nmf pca neural net database visualization mysql server cassandra hbase elasticsearch d3js dcjs plotly kibana ggplot tableau regular expression html css angular logstash kafka flask git docker computer vision open understanding deep learning datum science assurance associate ernst young llp javascript jquery fraud investigation dispute service technology assist review tar accelerate run analytic generate report core member help develop automate platform tool scratch discovery domain implement predictive coding result reduce labor cost spend lawyer understand end flow solution research development classification mining present text work analyze output precision monitoring entire code evidence follow standard classifier order identify red flag fraudrelated issue tfidf word2vec doc2vec cosine similarity bayes VADER TextBlob dashboard multiple project usa client motor vehicle customer receive feedback survey past perform positive negative neutral series comment category create heat map term base frequency extract plot cloud customize effective reporting chatbot friendly product simple question hour operation reservation option chat bot serve relate give overview recommendation response build chain relevant answer intelligence pipeline ask recommend processing nltk spacy governance organization informed store integrate portfolio synthesize unstructured data source facilitate action well position counter risk scan format parse file meta push index elastic search interactive preform rot helps content redundant outdated trivial fulltext predefine method tag pii personally identifiable social security number address name frequently target cyberattack investigative case fap inbuilt manager suite erp system interrogate accounting anomaly indicator advanced bootstrap NodeJS'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nðŸ“Œ Sample Resume Text After cleaning:\")\n",
    "cleaned_DataSet.iloc[0]['Resume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9ebfc5d5-4e73-40c8-83fc-593b87e6c740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 Most Frequent Words in Resumes:\n",
      "system: 94\n",
      "management: 83\n",
      "technology: 75\n",
      "database: 67\n",
      "datum: 66\n",
      "service: 66\n",
      "tool: 65\n",
      "application: 65\n",
      "software: 64\n",
      "engineering: 64\n",
      "computer: 63\n",
      "developer: 63\n",
      "development: 61\n",
      "report: 59\n",
      "business: 59\n",
      "design: 59\n",
      "develop: 58\n",
      "work: 58\n",
      "process: 56\n",
      "customer: 55\n",
      "create: 55\n",
      "sql: 53\n",
      "solution: 50\n",
      "base: 50\n",
      "manage: 50\n",
      "requirement: 47\n",
      "language: 46\n",
      "provide: 46\n",
      "server: 45\n",
      "implement: 45\n",
      "window: 45\n",
      "project: 44\n",
      "testing: 44\n",
      "knowledge: 44\n",
      "issue: 43\n",
      "test: 43\n",
      "java: 42\n",
      "science: 42\n",
      "analysis: 41\n",
      "web: 41\n",
      "activity: 41\n",
      "engineer: 40\n",
      "perform: 39\n",
      "relate: 38\n",
      "include: 38\n",
      "communication: 37\n",
      "involve: 37\n",
      "product: 36\n",
      "manager: 36\n",
      "lead: 36\n"
     ]
    }
   ],
   "source": [
    "all_words = ' '.join(cleaned_DataSet['Resume']).split()\n",
    "word_freq = Counter(all_words)\n",
    "#Top 50 Most Frequent Words in Resumes\n",
    "top_words = word_freq.most_common(50)\n",
    "print(\"Top 50 Most Frequent Words in Resumes:\")\n",
    "for word, freq in top_words:\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a810af-497b-439f-b526-8b8b57d83a13",
   "metadata": {},
   "source": [
    "## 6. Encode Target Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0842d6cb-22fd-488c-94d2-9ca547a2b900",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "cleaned_DataSet['Category'] = le.fit_transform(cleaned_DataSet['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "23497723-4602-4d26-9c05-7600f8da6d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Category encoding \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       6\n",
       "1       6\n",
       "2       6\n",
       "3       6\n",
       "4       6\n",
       "       ..\n",
       "161    23\n",
       "162    23\n",
       "163    23\n",
       "164    23\n",
       "165    23\n",
       "Name: Category, Length: 166, dtype: int32"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nðŸ“Œ Category encoding \")\n",
    "cleaned_DataSet['Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c4837-fe1a-4b8e-99dc-8a355a166da3",
   "metadata": {},
   "source": [
    "## 7. Optional: Oversample to Balance Categories\n",
    "# Comment/uncomment this section as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9b6e4d8c-48c4-4917-b987-9d06d69f511c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usife\\AppData\\Local\\Temp\\ipykernel_7844\\2583446912.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_df = cleaned_DataSet.groupby('Category').apply(lambda x: x.sample(max_size, replace=True)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "max_size = cleaned_DataSet['Category'].value_counts().max()\n",
    "balanced_df = cleaned_DataSet.groupby('Category').apply(lambda x: x.sample(max_size, replace=True)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b886ef9c-78b5-43b0-b845-7ed2a9238f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the balanced dataset\n",
    "cleaned_DataSet_balanced = balanced_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353ff2c-c674-4a6e-a81d-23bd954114ec",
   "metadata": {},
   "source": [
    "## 8. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "67c04501-7758-4999-8d12-977030eb7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaned_DataSet['Resume'].values\n",
    "y = cleaned_DataSet['Category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef81f0-4c2f-42f8-ad0b-7f7aa10b706e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e6cb6d36-1876-48f8-96ad-46ff04f788ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_balanced, X_test_balanced, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345affff-b0cf-4811-9dd3-ec5893bd9d8f",
   "metadata": {},
   "source": [
    "## 9. TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "1a0a3c79-2471-4c04-a55e-f1c0fe6de34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    stop_words='english',\n",
    "    max_features=1500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "410c2bd7-5684-4e0a-a0d0-5d592ab7feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = word_vectorizer.fit_transform(X_train_balanced)\n",
    "X_test_vec = word_vectorizer.transform(X_test_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "dd6241c1-2e5b-4c63-94ac-b68122163bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_bal, y_train_bal = ros.fit_resample(X_train_vec, y_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "85f1e426-c2e3-4add-9c0b-f6519682e00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1500)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b6697671-38dd-47ea-88f8-8c5172e28586",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer.fit(X)\n",
    "X_vectorized = word_vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "61acd3e4-9844-4da9-908f-7ee4836fcd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer.fit(X_balanced)\n",
    "X_balanced_vectorized = word_vectorizer.transform(X_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a8f78df5-ad1c-4748-9309-de53f386fa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… TF-IDF feature matrix shape: (166, 1500)\n",
      "\n",
      "âœ… TF-IDF feature balanced matrix shape: (166, 1500)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nâœ… TF-IDF feature matrix shape:\", X_vectorized.shape)\n",
    "print(\"\\nâœ… TF-IDF feature balanced matrix shape:\", X_balanced_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a500c9-4f62-463a-8d08-97d2c2079a41",
   "metadata": {},
   "source": [
    "## 10. Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0db4ec45-2b71-4775-9b4a-ae18fecd5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vectorized,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7cbca62c-57e3-4b62-851c-b384aa112979",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(\n",
    "    X_balanced_vectorized,\n",
    "    y_balanced,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "eb33d9cf-42a2-4e05-bf7a-e8eee9608da5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… Train shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Test shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… Train balanc shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_train_balanced\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\nâœ… Train shape:\", X_train.shape)\n",
    "print(\"âœ… Test shape:\", X_test.shape)\n",
    "print(\"\\nâœ… Train balanc shape:\", X_train_balanced.shape)\n",
    "print(\"âœ… Test balanc shape:\", X_test_balanced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623957d8-310b-4249-8deb-32cbc7e391cb",
   "metadata": {},
   "source": [
    "## 11. Save Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "96601a64-6c53-427d-a76f-54b430d7cbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/y_test.pkl']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the split data\n",
    "joblib.dump(X_train, '../models/X_train.pkl')\n",
    "joblib.dump(X_test, '../models/X_test.pkl')\n",
    "joblib.dump(y_train, '../models/y_train.pkl')\n",
    "joblib.dump(y_test, '../models/y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a8dd21de-a7b4-4380-afdf-d22144b3410b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/y_test_bal.pkl']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the split data balanced\n",
    "joblib.dump(X_train_bal, '../models/X_train_bal.pkl')\n",
    "joblib.dump(X_test_balanced, '../models/X_test_bal.pkl')\n",
    "joblib.dump(y_train_bal, '../models/y_train_bal.pkl')\n",
    "joblib.dump(y_test_balanced, '../models/y_test_bal.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "ad49b2d3-d0a5-4795-93bd-fd81df4f10db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<300x1500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 30173 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec9d28-801c-40ef-add1-ad23f5ed7ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_ai]",
   "language": "python",
   "name": "conda-env-env_ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
